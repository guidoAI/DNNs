{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a403548",
   "metadata": {},
   "source": [
    "# Deep neural networks\n",
    "\n",
    "Over the last 15 years, most major advances in AI have been achieved with the help of <I>deep</I> neural networks. This \"AI revolution\" started with deep neural networks outcompeting other approaches on computer vision tasks. However, over the years other feats have been added to this list, including beating the world champion in Go (something that was expected to still require decades of AI research), playing Atari games at expert human level, beating top players at StarCraft, predicting the 3d-structure of proteines, generating images based on textual inputs (DALL-E), and highly advanced chat bots (ChatGPT).\n",
    "\n",
    "Deep neural networks had actually been around for quite a long time, but they were generally impractical to train. This changed due to three main advances:\n",
    "<OL>\n",
    "    <LI>Hardware: Graphical Processor Units (GPUs) considerably sped up deep neural network operations.</LI>\n",
    "    <LI>More data: Digitalization led to larger data sets.</LI>\n",
    "    <LI>Algorithmic: Better learning methods and model structures.</LI>\n",
    "</OL>\n",
    "\n",
    "Adding layers to a neural network increases its representational power, but also lead to additional problems, including:\n",
    "<UL>\n",
    "    <LI>Hyperparameters like the learning rate can be hard and costly to tune.</LI>\n",
    "    <LI>The gradient used for learning can vanish or blow up over the layers.</LI>\n",
    "    <LI>Network activations can also vanish or blow up over the layers.</LI>\n",
    "    <LI>Trained networks may overfit and generalize badly to the test set.</LI>\n",
    "</UL>\n",
    "\n",
    "Here we will explore some of these learning problems and related solutions.\n",
    "\n",
    "## Tuning the learning rate\n",
    "\n",
    "As explained in the previous lecture, the core of neural network learning is still formed by gradient descent. \n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 1.</B></FONT> <BR>\n",
    "Below we create an artificial, quadratic loss function of a single parameter $w$. This parameter can be seen as the weight of a neural network. We perform straightforward gradient descent to minimize the loss function $\\mathcal{L}(w) = a w^2 + bw + c$. In the plot, the initial iterations are represented by dark blue markers, the later ones by bright yellow.\n",
    "<OL>\n",
    "    <LI>Play around with the learning rate. What happens when you make it really small? What happens when you give it a value of $0.9$, $1.0$, $1.01$ or even much larger?</LI>\n",
    "</OL>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc94a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fs = 15\n",
    "\n",
    "# illustrate gradient descent, adam:\n",
    "mx = 10\n",
    "step = 0.01\n",
    "\n",
    "# artificial loss function and its gradient:\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "w = np.arange(-mx, mx, step)\n",
    "loss_function = a * w ** 2 + b * w + c\n",
    "gradient_loss_function = 2 * a * w + b\n",
    "\n",
    "# gradient descent:\n",
    "lr = 0.9\n",
    "epsilon = 0.0001 # stop when close enough to the minimum\n",
    "wi = mx / 2 # starting point\n",
    "n_iterations = 100 # learning steps\n",
    "ws = []\n",
    "ys = []\n",
    "iterations = []\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # store the current point and iteration:\n",
    "    iterations.append(i)\n",
    "    ws.append(wi)\n",
    "    \n",
    "    # perform a learning update:\n",
    "    wi = wi - lr * (2 * a * wi + b)\n",
    "    \n",
    "    # store the loss\n",
    "    ys.append(a * wi ** 2 + b * wi + c)\n",
    "    \n",
    "    if(ys[i] < np.min(loss_function) + epsilon):\n",
    "        break\n",
    "        \n",
    "iterations = np.asarray(iterations)\n",
    "ws  = np.asarray(ws)\n",
    "ys = np.asarray(ys)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, loss_function, 'k')\n",
    "plt.scatter(ws, a * ws ** 2 + b * ws + c, c = iterations, cmap='viridis')\n",
    "plt.xlabel('$w$', fontsize=fs)\n",
    "plt.ylabel('$\\mathcal{L}(w)$', fontsize=fs)\n",
    "plt.title('Gradient descent, learning rate = ' + str(lr), fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a521adc",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "As you have seen in the exercise, \"vanilla\" gradient descent backpropagation can take a long time when the gradient is small and can overshoot when the gradient is large. One traditional solution to this has been to introduce <i>momentum</i>: <BR>\n",
    "    \n",
    "$\\mathbf{g}_t \\leftarrow \\rho \\mathbf{g}_{t-1} + (1-\\rho) \\mathbf{\\hat{g}}_t$\n",
    "\n",
    "The effect of this will be especially positive when the gradient is changing sign a lot, as for a learning rate of $0.9$. \n",
    "    \n",
    "<FONT COLOR=\"red\"><B>Exercise 2.</B></FONT> <BR>\n",
    "<OL>\n",
    "    <LI>Implement momentum for the gradient in the code below. Also print the number of iterations that were necessary to converge to the optimum. Use a learning rate of $0.9$. <B>Can you set $\\rho$ such that it decreases the number of necessary iterations?</B></LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacf3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fs = 15\n",
    "\n",
    "# illustrate gradient descent, adam:\n",
    "mx = 10\n",
    "step = 0.01\n",
    "\n",
    "# artificial loss function and its gradient:\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "x = np.arange(-mx, mx, step) # TODO: change this to w\n",
    "loss_function = a * x ** 2 + b * x + c\n",
    "gradient_loss_function = 2 * a * x + b\n",
    "\n",
    "# gradient descent:\n",
    "lr = 0.02\n",
    "epsilon = 0.0001 # stop when close enough to the minimum\n",
    "xi = mx / 2 # starting point\n",
    "n_iterations = 100 # learning steps\n",
    "xs = []\n",
    "ys = []\n",
    "iterations = []\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # store the current point and iteration:\n",
    "    iterations.append(i)\n",
    "    xs.append(xi)\n",
    "    \n",
    "    # perform a learning update:\n",
    "    # CHANGE THE FOLLOWING CODE LINE FOR IMPLEMENTING MOMENTUM:\n",
    "    xi = xi - lr * (2 * a * xi + b)\n",
    "    \n",
    "    # store the loss\n",
    "    ys.append(a * xi ** 2 + b * xi + c)\n",
    "    \n",
    "    if(ys[i] < np.min(loss_function) + epsilon):\n",
    "        break\n",
    "        \n",
    "iterations = np.asarray(iterations)\n",
    "xs  = np.asarray(xs)\n",
    "ys = np.asarray(ys)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, loss_function, 'k')\n",
    "plt.scatter(xs, a * xs ** 2 + b * xs + c, c = iterations, cmap='viridis')\n",
    "plt.xlabel('$w$', fontsize=fs)\n",
    "plt.ylabel('$\\mathcal{L}(w)$', fontsize=fs)\n",
    "plt.title('Gradient descent, learning rate = ' + str(lr), fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640824ab",
   "metadata": {},
   "source": [
    "## Adaptive moments (Adam)\n",
    "\n",
    "Adam is an algorithm that introduces momentum to the gradient in an adaptive way. Specifically, it estimates a first and a second order moment as follows:  <BR>\n",
    "\n",
    "$\\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1-\\rho_1) \\mathbf{g}$\n",
    "    \n",
    "$\\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1-\\rho_2) \\mathbf{g} \\odot \\mathbf{g}$\n",
    "\n",
    ", where $\\odot$ is the elementwise product. \n",
    "    \n",
    "Then, the moments are corrected to remove the estimation bias:\n",
    "    \n",
    "$\\mathbf{\\hat{s}} \\leftarrow \\frac{\\mathbf{s}}{1-\\rho^t_1}$\n",
    "\n",
    "$\\mathbf{\\hat{r}} \\leftarrow \\frac{\\mathbf{r}}{1-\\rho^t_2}$\n",
    "\n",
    ", where $t$ corresponds to the number of past updates. \n",
    "    \n",
    "The update of the parameters $\\mathbf{\\theta}$ then becomes:\n",
    "    \n",
    "$\\mathbf{\\Delta \\theta} = - \\alpha \\frac{\\mathbf{\\hat{s}}}{\\mathbf{\\hat{r}} + \\delta}$\n",
    "    \n",
    ", where $\\delta$ is a small number that prevents dividing by zero. The first order momentum $\\mathbf{s}$ is very similar to the momentum you implemented yourself in exercise 2. The role of the second order momentum $\\mathbf{r}$ is to reduce step size in directions with large gradient magnitude, and increase them in directions with small gradient magnitude. This should lead to quicker convergence and less overshoot.<BR>\n",
    "<BR>\n",
    "    \n",
    "    \n",
    "<CENTER>\n",
    "<IMG SRC=\"DALLÂ·E 2023-01-16 09.01.15 - A robot version of the first man, Adam, taking an apple from a tree with a snake .png\" WIDTH=\"400px\"></IMG>\n",
    "DALL-E's interpretation of the Adam algorithm.\n",
    "</CENTER>    \n",
    "    <BR>\n",
    "        \n",
    "<FONT COLOR=\"red\"><B>Exercise 3.</B></FONT> <BR>\n",
    "<OL>\n",
    "    <LI>Implement Adam in the code block below. Does it lead to quicker convergence? What happens when you set a large learning rate like $1.01$?</LI>\n",
    "</OL>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fs = 15\n",
    "\n",
    "# illustrate gradient descent, adam:\n",
    "mx = 10\n",
    "step = 0.01\n",
    "\n",
    "# artificial loss function and its gradient:\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "x = np.arange(-mx, mx, step) # TODO: change this to w\n",
    "loss_function = a * x ** 2 + b * x + c\n",
    "gradient_loss_function = 2 * a * x + b\n",
    "\n",
    "# gradient descent:\n",
    "lr = 0.9\n",
    "epsilon = 0.0001 # stop when close enough to the minimum\n",
    "xi = mx / 2 # starting point\n",
    "n_iterations = 100 # learning steps\n",
    "xs = []\n",
    "ys = []\n",
    "iterations = []\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # store the current point and iteration:\n",
    "    iterations.append(i)\n",
    "    xs.append(xi)\n",
    "    \n",
    "    # perform a learning update:\n",
    "    # CHANGE THE FOLLOWING CODE LINE FOR IMPLEMENTING ADAM: \n",
    "    xi = xi - lr * (2 * a * xi + b)\n",
    "    \n",
    "    # store the loss\n",
    "    ys.append(a * xi ** 2 + b * xi + c)\n",
    "    \n",
    "    if(ys[i] < np.min(loss_function) + epsilon):\n",
    "        break\n",
    "        \n",
    "iterations = np.asarray(iterations)\n",
    "xs  = np.asarray(xs)\n",
    "ys = np.asarray(ys)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, loss_function, 'k')\n",
    "plt.scatter(xs, a * xs ** 2 + b * xs + c, c = iterations, cmap='viridis')\n",
    "plt.xlabel('$w$', fontsize=fs)\n",
    "plt.ylabel('$\\mathcal{L}(w)$', fontsize=fs)\n",
    "plt.title('Gradient descent, learning rate = ' + str(lr), fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80a2e9",
   "metadata": {},
   "source": [
    "## Early stopping with a validation set\n",
    "\n",
    "The goal of learning is not only to perform well on the data that is used for training, but to also perform well on data that has not yet been seen by the learning algorithm. If a neural network is able to also make good predictions for unseen data, then it is said to \"generalize\" well. As stated earlier, the deeper a neural network is, the more representative power it has. The advantage of this is that a deeper neural network can learn more complex functions. The disadvantage is that it can adjust itself to noise in the input data. This problem is aggravated when little data is available for training.\n",
    "\n",
    "One way to prevent neural networks to overfit on the training data, is to divide the samples in a \"training set\" and a \"validation set\". The training set is used for adapting the weights with backpropagation, minimizing the error on the training samples. Each epoch the neural network is also applied to the validation set. When the network starts to overfit on the training data at the cost of generalization, the loss on the validation set will start to increase. The error on the validation set is not used for adapting the weights, but for determining when to stop training. \n",
    "\n",
    "Given a set of data samples, there is typically an additional \"test set\", which is not used during training at all. The loss on this test set can be reported to illustrate the true generalization performance. Of course, as soon as a Machine Learning practitioner uses the test set to adapt parameters of the learning algorithm, the test set has actually played a role in training. Then it looses its role as an independent test of the learned network.\n",
    "\n",
    "<CENTER>\n",
    "<IMG SRC=\"training_validation_test_split.png\" WIDTH=\"400px\"></IMG>\n",
    "Illustration of a subdivision of a data set in a training, validation, and test set.\n",
    "</CENTER>\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 4.</B></FONT>\n",
    "\n",
    "Below we coded a linear regression example, where we added normally distributed noise to the targets in the data set. We then split the data set in a training, validation and test set. In this case, we have purposively made the training set rather small, the validation set slightly bigger, and we have quite a large test set. Normally, one would do the opposite: Use a large part of the data for training and a much smaller part for validation and testing.\n",
    "\n",
    "We use the ``torch.nn.Sequential`` method for creating a standard MLP. Also notice how the Adam optimizer is implemented here, simply by setting it as the optimizer: ``optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)``.\n",
    "\n",
    "<OL>\n",
    "    <LI>Implement a scheme that stops the training when the validation loss increases. <i>Hint:</i> You can low pass filter the validation loss to this end.</LI>\n",
    "    <LI>What happens to the average training, validation, and test loss?</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Whether to stop when the validation error increases:\n",
    "stop_when_val_error_increases = False\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "seed_number = 16012023\n",
    "torch.manual_seed(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "random.seed(seed_number)\n",
    "\n",
    "# create the data:\n",
    "limit = 3\n",
    "n_steps = 1000\n",
    "x = torch.linspace(-limit, limit, n_steps)\n",
    "x = torch.reshape(x, (n_steps, 1))\n",
    "y_no_noise = 0.2 * x\n",
    "noise = 0.2 * torch.randn(x.shape)\n",
    "y = y_no_noise + noise\n",
    "\n",
    "# Split data into training, validation and test sets\n",
    "n_training = 10\n",
    "n_val = 30\n",
    "inds = np.random.choice(n_steps, n_training, replace=False)\n",
    "inds = np.sort(inds)\n",
    "x_training = x[inds]\n",
    "y_training = y[inds]\n",
    "remaining_inds = np.setdiff1d(np.arange(n_steps), inds)\n",
    "val_inds = np.random.choice(remaining_inds, n_val, replace=False)\n",
    "val_inds = np.sort(val_inds)\n",
    "x_val = x[val_inds]\n",
    "y_val = y[val_inds]\n",
    "remaining_inds = np.setdiff1d(remaining_inds, val_inds)\n",
    "remaining_inds = np.sort(remaining_inds)\n",
    "x_test = x[remaining_inds]\n",
    "y_test = y[remaining_inds]\n",
    "n_test = len(x_test)\n",
    "\n",
    "# Make a neural network model for the MLP with sigmoid activation functions in the hidden layer, and linear on the output\n",
    "n_hidden_neurons = 100\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                            torch.nn.Sigmoid(),torch.nn.Linear(n_hidden_neurons, 1))\n",
    "\n",
    "# We will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 1000\n",
    "show_plots = False\n",
    "show_plot_every = int(n_epochs / 5)\n",
    "\n",
    "training_loss = np.zeros(n_epochs)\n",
    "validation_loss = np.zeros(n_epochs)\n",
    "\n",
    "for t in range(n_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_training)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_training)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    training_loss[t] = loss.item()\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Determine validation loss\n",
    "    y_val_pred = model(x_val)\n",
    "    val_loss = loss_fn(y_val_pred, y_val)\n",
    "    validation_loss[t] = val_loss.item()\n",
    "    \n",
    "    #if(stop_when_val_error_increases):\n",
    "        \n",
    "        # IMPLEMENT YOUR CODE HERE\n",
    "        \n",
    "        #print('Validation loss increased, stopping training')\n",
    "        #break\n",
    "\n",
    "    if show_plots and t % show_plot_every == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(x, y_no_noise, 'k--')\n",
    "        plt.plot(x_training, y_training, 'ko')\n",
    "        plt.plot(x_training, y_pred.detach().numpy())\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend(['Ground truth', 'Data', 'Fit'])\n",
    "        plt.show()\n",
    "\n",
    "# Plot the final state of affairs:\n",
    "\n",
    "mode_string = 'Validation set stopping' if stop_when_val_error_increases else 'No validation set'\n",
    "y_pred = model(x_training)\n",
    "y_pred_test = model(x_test)\n",
    "test_loss = loss_fn(y_pred_test, y_test)\n",
    "loss_string = \"{:.2f}\".format(test_loss.item())\n",
    "title_string = mode_string + ', test loss: ' + loss_string\n",
    "print('Test loss: ', test_loss.item())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y_no_noise, 'k--')\n",
    "plt.plot(x_training, y_training, 'ro')\n",
    "plt.plot(x_val, y_val, 'go')\n",
    "plt.plot(x_training, y_pred.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ground truth', 'Training data', 'Validation data', 'Fit'])\n",
    "plt.title(title_string)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the loss over time:\n",
    "plt.figure()\n",
    "plt.plot(training_loss, 'r')\n",
    "plt.plot(validation_loss, 'g')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.show()\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# Plot the final state of affairs to compare SGD and Adam:\n",
    "plt.figure()\n",
    "plt.plot(x, y_no_noise, 'k--')\n",
    "plt.plot(x_training, y_training, 'ko')\n",
    "plt.plot(x_training, y_pred.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ground truth', 'Data', 'Fit'])\n",
    "plt.show()\n",
    "\n",
    "y_pred = model(x_training)\n",
    "loss = loss_fn(y_pred, y_training)\n",
    "print(f'Average loss on the training set: {loss.item()/n_training:.2f}')\n",
    "\n",
    "y_pred = model(x_val)\n",
    "loss = loss_fn(y_pred, y_val)\n",
    "print(f'Average loss on the validation set: {loss.item()/n_val:.2f}')\n",
    "\n",
    "y_pred = model(x_test)\n",
    "loss = loss_fn(y_pred, y_test)\n",
    "print(f'Average loss on the test set: {loss.item()/n_test:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770de73",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "A well-proven method of increasing generalization and robustness is to use <i>ensembles</i> of neural networks. In ensemble learning, a group of neural networks is trained on different parts of the training set (see below for NN1-3). \n",
    "\n",
    "<IMG SRC=\"ensemble_training_set.png\" WIDTH=\"200px\"/>\n",
    "\n",
    "Because the different networks train on different parts of the training data, they may give different outcomes for the same input sample, i.e., it can be that $\\hat{y}_i \\neq \\hat{y}_j$, $i \\neq j$ (see below). This allows for voting in classification problems or averaging in regression problems. \n",
    "\n",
    "<IMG SRC=\"ensemble_prediction.png\" WIDTH=\"200px\"/>\n",
    "\n",
    "Ensemble learning has two main advantages. First, even though individual networks may overfit on their part of the training data, using an ensemble of networks will reduce the influence overfitting has on the final outcome. In other words, the hope of ensemble learning lies in a \"wisdom of the crowd\". Second, having multiple outputs for the same input sample also allows for evaluating <i>uncertainty</i>, e.g., by looking at the variance of the individual network outputs. Knowing how much to trust a given output can be very important for many applications. \n",
    "\n",
    "It also has a distinct disadvantage: Ensemble learning costs more time not only for training, but also for execution during inference time. This may be prohibitive for, e.g., embedded applications such as autonomous robots that rely on limited, onboard processors.<BR><BR>\n",
    "\n",
    "\n",
    "<CENTER>\n",
    "    <IMG SRC=\"DALLÂ·E 2023-03-06 11.21.22 - a group of robots voting.png\" WIDTH=\"400px\"/>\n",
    "    An ensemble of robots voting by DALL-E.\n",
    "</CENTER>\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 5.</B></FONT>\n",
    "\n",
    "<OL>\n",
    "    <LI> Below we give the same code as before, in which a single network has a very small training set to which it will overfit. However, now we implemented the training of an ensemble of networks. Try out various numbers of `N_ensemble`. How many nets suffice to start approximating the ground truth function well?\n",
    "    </LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea44d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# CHANGE THE NUMBER OF NETWORKS IN THE ENSEMBLE:\n",
    "N_ensemble = 1\n",
    "\n",
    "# Whether to stop when the validation error increases:\n",
    "stop_when_val_error_increases = False\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "seed_number = 16012023\n",
    "#torch.random.seed = seed_number\n",
    "torch.manual_seed(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "random.seed(seed_number)\n",
    "\n",
    "limit = 3\n",
    "n_steps = 1000\n",
    "x = torch.linspace(-limit, limit, n_steps)\n",
    "x = torch.reshape(x, (n_steps, 1))\n",
    "y_no_noise = 0.2 * x\n",
    "noise = 0.2 * torch.randn(x.shape)\n",
    "y = y_no_noise + noise\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(N_ensemble):\n",
    "    # Split data into training, validation and test sets\n",
    "    n_training = 10\n",
    "    n_val = 30\n",
    "    inds = np.random.choice(n_steps, n_training, replace=False)\n",
    "    inds = np.sort(inds)\n",
    "    x_training = x[inds]\n",
    "    y_training = y[inds]\n",
    "    remaining_inds = np.setdiff1d(np.arange(n_steps), inds)\n",
    "    val_inds = np.random.choice(remaining_inds, n_val, replace=False)\n",
    "    val_inds = np.sort(val_inds)\n",
    "    x_val = x[val_inds]\n",
    "    y_val = y[val_inds]\n",
    "    remaining_inds = np.setdiff1d(remaining_inds, val_inds)\n",
    "    remaining_inds = np.sort(remaining_inds)\n",
    "    x_test = x[remaining_inds]\n",
    "    y_test = y[remaining_inds]\n",
    "\n",
    "    # Make a neural network model for the MLP with sigmoid activation functions in the hidden layer, and linear on the output\n",
    "    n_hidden_neurons = 100\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                                torch.nn.Sigmoid(),torch.nn.Linear(n_hidden_neurons, 1))\n",
    "\n",
    "    # We will use Mean Squared Error (MSE) as our loss function.\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    n_epochs = 1000\n",
    "    show_plots = False\n",
    "    show_plot_every = int(n_epochs / 5)\n",
    "    training_loss = np.zeros(n_epochs)\n",
    "    validation_loss = np.zeros(n_epochs)\n",
    "    val_low_pass = 0\n",
    "\n",
    "    for t in range(n_epochs):\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_training)\n",
    "\n",
    "        # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "        # values of y, and the loss function returns a Tensor containing the\n",
    "        # loss.\n",
    "        loss = loss_fn(y_pred, y_training)\n",
    "        if t % 100 == 0:\n",
    "            print(t, loss.item())\n",
    "\n",
    "        training_loss[t] = loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Determine validation loss\n",
    "        y_val_pred = model(x_val)\n",
    "        val_loss = loss_fn(y_val_pred, y_val)\n",
    "        validation_loss[t] = val_loss.item()\n",
    "        prev_val = val_low_pass\n",
    "        if(t == 0):\n",
    "            val_low_pass = val_loss.item()\n",
    "        else:\n",
    "            val_low_pass = 0.9 * val_low_pass + 0.1 * val_loss.item()\n",
    "        if(stop_when_val_error_increases and t > 0 and val_low_pass > prev_val):\n",
    "            print('Validation loss increased, stopping training')\n",
    "            break\n",
    "\n",
    "        if show_plots and t % show_plot_every == 0:\n",
    "            plt.figure()\n",
    "            plt.plot(x, y_no_noise, 'k--')\n",
    "            plt.plot(x_training, y_training, 'ko')\n",
    "            plt.plot(x_training, y_pred.detach().numpy())\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.legend(['Ground truth', 'Data', 'Fit'])\n",
    "            plt.show()\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# Plot the final state of affairs:\n",
    "\n",
    "mode_string = 'Validation set stopping' if stop_when_val_error_increases else 'No validation set'\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y_no_noise, 'k--', label = 'Ground truth')\n",
    "\n",
    "y_preds = torch.zeros((n_steps, N_ensemble))\n",
    "for i in range(N_ensemble):\n",
    "    model = models[i]\n",
    "    y_preds[:, i] = model(x).squeeze()\n",
    "    plt.plot(x, y_preds[:,i].detach().numpy(), color=[0.8,0.8,0.8], linewidth=0.5, label='_hidden_')\n",
    "\n",
    "y_pred = torch.mean(y_preds, dim=1)\n",
    "loss = loss_fn(y_pred, y_no_noise) / n_steps\n",
    "loss_string = \"{:.2f}\".format(loss.item())\n",
    "\n",
    "title_string = mode_string + ', loss: ' + loss_string + ', ensemble size: ' + str(N_ensemble)\n",
    "print('Loss: ', loss.item())\n",
    "\n",
    "\n",
    "plt.plot(x, y_pred.detach().numpy(), label='Ensemble fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title(title_string)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af2f07",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A new way to improve the robustness of learning and the generalization to unseen data is <i>dropout</i>. If dropout is used for a layer, neurons are set to zero with a given probability $p$. Normally, dropout is only used during training. When evaluating the trained network on test samples, dropout is then switched off. Of course, this means that at test time the activations will be higher. In order to compensate this, during training the outgoing weights of the layer are multiplied by $\\frac{1}{p}$. Suppose that $p=0.5$, during training on average $50\\%$ of the neurons are set to zero, while the weights are multiplied by 2. After training neurons will no longer be dropped out, and the weights will no longer be scaled. To make this happen, in PyTorch the neural network model should be set to evaluation mode: ``model.eval()``. Else, dropout will remain active.\n",
    "\n",
    "<CENTER>\n",
    "    <IMG SRC=\"DALLÂ·E 2023-01-16 09.15.02 - A robot highschool dropout.png\" WIDTH=\"400px\"/>\n",
    "    A  robot high school dropout by DALL-E.\n",
    "</CENTER>\n",
    "\n",
    "In a way, dropout is similar to <i>ensemble learning</i>, in which group of neural networks is trained by presenting them different parts of the training data set. In ensemble learning, at test time all different, trained neural networks are applied to the same test sample. This generates statistics for the prediction. For example, the average can be used as the single ensemble estimate. The variance can be used as an indication of prediction uncertainty.\n",
    "\n",
    "Dropout is similar to ensemble learning, because dropping out different neurons in the network actually boils down to running a sub-network with a different connection structure for each training sample. Of course, in contrast to ensemble learning, a neural network with dropout has a single set of weights (so weights are shared in the \"ensemble\"), and the network is trained on a single training data set.\n",
    "\n",
    "In fact, dropout can be sustained at test time on purpose in order to generate multiple, different predictions for the same sample. Again, the statistics can then be used for getting not only a robust single estimate, but also the uncertainty of this estimate.\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 6.</B></FONT>\n",
    "\n",
    "Below we set up a regression task and a neural network, depending on the variable ``drop_out`` with or without a dropout layer. A dropout layer is made in PyTorch with the command: ``torch.nn.Dropout(p=0.3)`` (setting $p = 0.3$ in this case).\n",
    "\n",
    "<OL>\n",
    "    <LI>Train the model without dropout first. How well does the network generalize outside of the training data interval, i.e., $x  \\in [-3, 3]$? Is there a way in which we can see that we can rely less on the network in that area?</LI>\n",
    "    <LI>Train the model with dropout. Keep dropout active after training, and run it 30 times on each sample. For each test sample, plot all the individual estimates with blue markers, and put the average in the variable y_test_mean. Does this provide a way to evaluate the uncertainty of the network outside the training domain?</LI>\n",
    "</OL>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CHANGE THIS TO FALSE / TRUE\n",
    "drop_out = False\n",
    "\n",
    "# Create Tensors to hold input and target outputs.\n",
    "limit = 3\n",
    "n_steps = 1000\n",
    "x = torch.linspace(-limit, limit, n_steps)\n",
    "x = torch.reshape(x, (n_steps, 1))\n",
    "\n",
    "def GT_func(x, a=1, b= 0, c=0):\n",
    "    return a*x**2 + b*x + c\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "y = GT_func(x, a, b, c)\n",
    "\n",
    "# Make a neural network model for the MLP with sigmoid activation functions in the hidden layer, and linear on the output\n",
    "n_hidden_neurons = 30\n",
    "p_drop_out = 0.2\n",
    "\n",
    "# Employ dropout or not:\n",
    "if(drop_out):\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                                torch.nn.Sigmoid(),torch.nn.Dropout(p=p_drop_out),\n",
    "                                torch.nn.Linear(n_hidden_neurons, 1))\n",
    "else:\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                                torch.nn.Sigmoid(), torch.nn.Linear(n_hidden_neurons, 1))\n",
    "\n",
    "# Learning parameters:\n",
    "\n",
    "# We will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-5\n",
    "n_epochs = 10000\n",
    "\n",
    "show_intermediate_plots = False\n",
    "\n",
    "# Train:\n",
    "n_epochs_print = 1000\n",
    "im = 0\n",
    "for t in range(n_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    x_in = x\n",
    "    y_pred = model(x_in)\n",
    "    y_target = y\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_target)\n",
    "    if t % n_epochs_print == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    if t % n_epochs_print == 0 and show_intermediate_plots:\n",
    "        h = plt.figure()\n",
    "        plt.plot(x, y, 'k--')\n",
    "        plt.plot(x_in, y_pred.detach().numpy())\n",
    "        text = f'Epoch {t:4d}'\n",
    "        plt.text(0, 0.8*np.max(np.asarray(y)), text)\n",
    "        plt.show()\n",
    "\n",
    "# test the network also outside of the input domain in the training set:\n",
    "limit_factor = 2\n",
    "x_test = torch.linspace(-limit_factor*limit, limit_factor*limit, n_steps)\n",
    "x_test = torch.reshape(x_test, (n_steps, 1))\n",
    "\n",
    "if(drop_out):\n",
    "    n_samples = 30\n",
    "    \n",
    "    # YOUR CODE:\n",
    "    \n",
    "else:\n",
    "    y_test_mean = model(x_test)\n",
    "    \n",
    "y_test_GT = GT_func(x_test, a, b, c)\n",
    "\n",
    "h = plt.figure()\n",
    "if(drop_out):\n",
    "    for i in range(n_samples):\n",
    "        plt.plot(x_test, y_test[:,i].detach().numpy(), 'bo', alpha=0.1)\n",
    "plt.plot(x_test, y_test_mean.detach().numpy(), 'r')\n",
    "plt.plot(x_test, y_test_GT, 'k--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aecf0c",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "Measurements of neurons in cats' visual systems have shown that they have neurons that specialize in reacting to elementary patterns like oriented edges in a small area of the visual field. The spatial structure of the retina is maintained in these areas, i.e., they are retinotopic. So there are neurons over the whole field of view that react to specific patterns. Whereas some neurons react for example to an oriented edge of $\\sim 0^{\\circ}$, others react to and edge at $\\sim 20^{\\circ}$, etc. \n",
    "\n",
    "Inspired by such neuroscientific findings, <i>convolutional neural networks</i> have been proposed. A convolutional layer is defined by a number of <i>feature kernels</i> that are convolved with the input pattern. The common mathematical formulation of a convolution is:\n",
    "\n",
    "$f(t) * g(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau$\n",
    "\n",
    ", where $g(t)$ is the kernel. The minus sign before $\\tau$ within the $g$ function means that the kernel is horizontally flipped. The reason for this lies in ascertaining commutativity, i.e., that $f*g = g*f$. For the use of convolution in neural network libraries, this property is not so important. \n",
    "\n",
    "In fact, in most neural network libraries, what is called 'convolution' is actually <i>cross-correlation</i>, which is defined as follows:\n",
    "\n",
    "$f(t) \\star g(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau$\n",
    "\n",
    "And, discretized:\n",
    "\n",
    "$f(t) \\star g(t) = \\sum_{i=-\\infty}^{\\infty} f(i) g(t+i)$\n",
    "\n",
    "Here, the kernel is not flipped. \n",
    "\n",
    "A common implementation of a 2D 'convolution' in neural network libraries is:\n",
    "\n",
    "$S(i,j) = (K*I)(i,j) = \\sum_m \\sum_n I(i+m,  j+n) K(m,n)$,\n",
    "\n",
    "with $I$ for example a 2D image (e.g., $1280 \\times 1024$ pixels) and $K$ a small kernel (e.g., $5 \\times 5$ pixels). The kernel values are 'weight' parameters of the convolutional neural network. Since these weights do not change with position when convolving the entire image with the kernel, this is referred to as 'weight sharing'. This highlights the contrast with a fully connected network that would have a weight from each input pixel to each hidden neuron. One of the reasons behind the success of convolutional neural networks is that this weight sharing drastically reduces the number of parameters that have to be learned. Imagine that a single hidden neuron fully connected to the input image would already require $1280 \\times 1024 = 1,310,720$ weights, whereas a single kernel of $5 \\times 5$ pixels only requires $25$ weights... It will give a response over the entire image, thus determining the activations of $1280 \\times 1024$ hidden neurons.\n",
    "\n",
    "Of course, when an image is in color and has three channels it can be convolved with 3D-kernels (e.g., $5 \\times 5 \\times 3$ pixels). Convolving a $1280 \\times 1024 \\times 3$ image with $n$ such 3D feature kernels while keeping the original image size will result in a hidden layer of dimensions $1280 \\times 1024 \\times n$. This itself can be convolved again with kernels of size, e.g., $3 \\times 3 \\times n$. Hence, by adding layers, you will get convolutions of convolutions of convolutions, and soforth. In this way a hierarchy of features is created, where the feature kernels represent ever more complex shapes when going deeper into the network. \n",
    "\n",
    "Moreover, typically, a convolutional layer is followed by a way of downsampling. An example of downsampling is <i>max-pooling</i>. In max-pooling, only the maximal value is retained in a region of interest, e.g., of size $2\\times2$. This reduces the size of the hidden layer, and increases the spatial scale of the features. Hence, at deeper layers in a convolutional neural network, features are not only more complex, but also of larger spatial scale. Finally, max-pooling also helps the network to deal with small translations, as the precise location in the region of interest does not matter.\n",
    "\n",
    "For regression and classification tasks, it is common to <i>flatten</i> the last hidden layer. This means transforming the $M \\times N \\times O$ structured hidden layer to a one-dimensional vector of length $MNO$. This transformation preserves spatial information, as the order of values in the vector is related to their place in the hidden layer. However, this layer is typically connected to another hidden layer or output layer in a fully connected way. Hence, the neurons in the subsequent layer can combine information from any location in the image. A very simple convolutional neural network is shown below.<BR><BR>\n",
    "\n",
    "<CENTER>\n",
    "    <IMG SRC=\"nn7.png\" WIDTH=\"800px\"/>\n",
    "    Example of a very small convolutional neural network.\n",
    "</CENTER>\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 7.</B></FONT><BR>\n",
    "In this exercise, we will train a convolutional neural network to recognize written digits that come from the \"MNIST\" data set. First please run the code below to load the data set and show some example images, together with their labels (shown in the title of each sub plot). Running the code again will load the same set but show different example images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    #target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    #target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "show_data = True\n",
    "if(show_data):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "        img, label = training_data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(str(label))\n",
    "        #plt.title(str(torch.argmax(label).item()))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e3931",
   "metadata": {},
   "source": [
    "Now we will define the convolutional neural network. Please check that this network is the same as the example CNN shown in the figure in the text above. Also note that we need to indicate the number of neurons that remain after the max-pooling operation $3136$. Can you calculate this number yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        in_channel = 1\n",
    "        out_channel = 16\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding ='same') # bias = False\n",
    "        self.act1 = nn.ReLU(inplace= True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear2 = nn.Linear(3136,512)\n",
    "        self.act2 = nn.ReLU(inplace= True)\n",
    "        self.linear3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.feature_maps = self.conv1(x)\n",
    "        tmp = self.act1(self.feature_maps)\n",
    "        tmp = self.maxpool1(tmp)\n",
    "        tmp = torch.flatten(tmp, 1, -1)\n",
    "        tmp = self.linear2(tmp)\n",
    "        tmp = self.act2(tmp)\n",
    "        logits = self.linear3(tmp)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef74e1d",
   "metadata": {},
   "source": [
    "In the following, we train the network. Please remark that we define a training and test loop that makes use of 'data loaders'. These data loaders automatically make the training batches used for a single training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Learning settings: \n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c89ec4",
   "metadata": {},
   "source": [
    "After training, we can select a random sample (here from the training set) and apply the network to it. Does the network recognize the digit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c28475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "img, label = training_data[sample_idx]\n",
    "X = img\n",
    "X = torch.reshape(X, (1,1,28,28))\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred.item()}\")\n",
    "\n",
    "\n",
    "# show the image\n",
    "plt.figure()\n",
    "img = img.detach().numpy()\n",
    "img = img.reshape((28,28))\n",
    "plt.imshow(1-img,  cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd69ec",
   "metadata": {},
   "source": [
    "<OL>\n",
    "    <LI> Can you make changes to the network and learning procedure to get a better performance? Does it help to create multiple convolutional layers?</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63841d33",
   "metadata": {},
   "source": [
    "## Fully convolutional networks\n",
    "\n",
    "Fully convolutional networks are networks that first use convolutions to extract features and subsequently use deconvolutions (or transposed convolutions) in order to get back to the original input size. Typically, fully convolutional neural networks are used to map an image to an image-sized output, such as an image segmentation or even just a reconstruction of the input image. Concerning this latter case, it may initially seem strange to train a network to reconstruct the input image from the input image. However, the convolutional part of the network results in a low number of hidden neurons in the middle of the fully convolutional network that <i>encode</i> for the inputs. Hence, the activations of the hidden neurons form a compressed <i>latent space</i> representation of the image, which can be used for other purposes. For instance, in robotics, this latent space can be used for reinforcement learning.\n",
    "\n",
    "<CENTER>\n",
    "<IMG SRC=\"FCNN.png\"></IMG><BR/>\n",
    "The fully convolutional neural network used in exercise 8.\n",
    "</CENTER>\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 8.</B></FONT><BR>\n",
    "Here, we will train an auto-encoder using the images of the MNIST dataset. First run the code block below, in order to (re)load MNIST, change the targets to the input images, and create a class for the fully convolutional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b39050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (re)load the MNIST data:\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    #target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    #target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "# For auto-encoder, change the targets in the training data:\n",
    "training_x = torch.FloatTensor(len(training_data), 28, 28)\n",
    "training_y = torch.FloatTensor(len(training_data), 28, 28)\n",
    "training_labels = np.zeros((len(training_data), 1))\n",
    "for i in range(len(training_data)):\n",
    "    img, label = training_data[i]\n",
    "    training_x[i] = img\n",
    "    training_y[i] = img\n",
    "    training_labels[i] = label\n",
    "training_x = training_x.reshape((len(training_data), 1, 28, 28))\n",
    "training_y = training_y.reshape((len(training_data), 1, 28, 28))\n",
    "\n",
    "test_x = torch.FloatTensor(len(test_data), 28, 28)\n",
    "test_y = torch.FloatTensor(len(test_data), 28, 28)\n",
    "test_labels = np.zeros((len(training_data), 1))\n",
    "for i in range(len(test_data)):\n",
    "    img, label = test_data[i]\n",
    "    test_x[i] = img\n",
    "    test_y[i] = img\n",
    "    test_labels[i] = label\n",
    "test_x = test_x.reshape((len(test_data), 1, 28, 28))\n",
    "test_y = test_y.reshape((len(test_data), 1, 28, 28))\n",
    "\n",
    "# function to show an image and its reconstruction:\n",
    "def show_img_and_reconstr(img, reconstr):\n",
    "    # show the image\n",
    "    plt.figure()\n",
    "    # show two subplots:\n",
    "    plt.subplot(1,2,1)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = img.reshape((28,28))\n",
    "    plt.imshow(1-img,  cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.subplot(1,2,2)\n",
    "    reconstr = reconstr.detach().cpu().numpy()\n",
    "    reconstr = reconstr.reshape((28,28))\n",
    "    plt.imshow(1-reconstr,  cmap='gray')\n",
    "    plt.title('Reconstructed')\n",
    "    plt.show()\n",
    "\n",
    "# class of fully convolutional network:\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        in_channel = 1\n",
    "        out_channel = 16\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=5, padding ='same') # bias = False\n",
    "        self.act1 = nn.ReLU(inplace= True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear2 = nn.Linear(3136,100)\n",
    "        self.act2 = nn.ReLU(inplace= True)\n",
    "        # self.linear3 = nn.Linear(512, 100)\n",
    "        # self.act3 = nn.ReLU(inplace= True)\n",
    "        # self.linear4 = nn.Linear(100, 512)\n",
    "        # self.act4 = nn.ReLU(inplace= True)\n",
    "        self.linear5 = nn.Linear(100, 2304)\n",
    "        self.act5 = nn.ReLU(inplace= True)\n",
    "        self.resh = nn.Unflatten(1, (16, 12, 12))\n",
    "        self.deconv = nn.ConvTranspose2d(out_channel, in_channel, kernel_size=6, stride = 2, padding_mode ='zeros')\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # convolutional pipeline:\n",
    "        self.feature_maps = self.conv1(x)\n",
    "        tmp = self.act1(self.feature_maps)\n",
    "        tmp = self.maxpool1(tmp)\n",
    "        tmp = torch.flatten(tmp, 1, -1)\n",
    "        tmp = self.linear2(tmp)\n",
    "        tmp = self.act2(tmp)\n",
    "        self.latent = tmp\n",
    "        # tmp = self.linear3(tmp)\n",
    "        # self.latent = self.act3(tmp)\n",
    "        # # deconvolutional pipeline:\n",
    "        # tmp = self.linear4(self.latent)\n",
    "        # tmp = self.act4(tmp)\n",
    "        tmp = self.linear5(tmp)\n",
    "        tmp = self.act5(tmp)\n",
    "        tmp = torch.flatten(tmp, 1, -1)\n",
    "        tmp = self.resh(tmp)\n",
    "        logits = self.deconv(tmp)\n",
    "        logits = self.sigmoid(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Train and test loop for the auto-encoder:\n",
    "def train_loop(training_x, training_y, batch_size, model, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(training_x)\n",
    "    \n",
    "    # shuffle the indices to get random batches:\n",
    "    inds = np.random.permutation(size)\n",
    "    \n",
    "    num_batches = size // batch_size\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        inds_batch = inds[batch*batch_size:(batch+1)*batch_size]\n",
    "        X = training_x[inds_batch]\n",
    "        y = training_y[inds_batch]\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(text_x, test_y, model, loss_fn):\n",
    "    size = len(test_x)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(size):\n",
    "            X = test_x[i]\n",
    "            X = X.reshape((1,1,28,28))\n",
    "            pred = model(X)\n",
    "            Y = test_y[i]\n",
    "            Y = Y.reshape((1,1,28,28))\n",
    "            indiv_loss = loss_fn(pred, Y).item()\n",
    "            test_loss += indiv_loss\n",
    "\n",
    "    test_loss /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d8496",
   "metadata": {},
   "source": [
    "The code below creates a fully convolutional network and trains it. After training, you can investigate how good the network reconstructs input images by running the code block further below.\n",
    "\n",
    "Are the current settings good enough? To find out, run the code blocks below. If not, please change one of the learning settings to fix the problem. Which one leads to the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8aa7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an FCNN:\n",
    "model = FCNN()\n",
    "\n",
    "learning_rate = 5e-3\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train = True\n",
    "if(train):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_x, training_y, batch_size, model, loss_fn, optimizer)\n",
    "        test_loop(test_x, test_y, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f424df",
   "metadata": {},
   "source": [
    "You can test the trained network by running the following code. It will show example image reconstructions. Do the input and output images look alike? If not, change the learning or network setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04230aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a number of random samples with their reconstructions:\n",
    "n_samples = 5\n",
    "for sam in range(n_samples):\n",
    "    sample_idx = torch.randint(len(training_x), size=(1,)).item()\n",
    "    img = training_x[sample_idx]\n",
    "    img = torch.reshape(img, (1,1,28,28))\n",
    "    reconstr = model(img)\n",
    "    loss = loss_fn(img, reconstr).item()\n",
    "    print(f'Loss = {loss}')\n",
    "    show_img_and_reconstr(img, reconstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c132d50",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 1.</B></FONT> <BR>\n",
    "In this example, tiny learning rates (e.g., $10^{-4}$) will stay close to the initial point. Small learning rates (e.g., $10^-2$) will lead to a nice convergence. Large learning rates like $0.9$ will continuously overshoot the optimum but still converge eventually. A learning rate of $1$ represents a bifurcation point to diverging learning results - itself resulting in ever the same parameters on both sides of the optimum. At larger learning rates, the parameters will diverge.\n",
    "    \n",
    "<FONT COLOR=\"red\"><B>Exercise 2.</B></FONT> <BR>\n",
    "Momentum can improve the number of iterations required to arrive at the optimum when the learning rate is $0.9$. A quick manual optimization shows that $\\rho = 0.1$ gives a significant improvement over no momentum ($\\rho = 0$). A momentum that is too large (e.g., $\\rho = 0.9$) actually worsens the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fs = 15\n",
    "\n",
    "# illustrate gradient descent, adam:\n",
    "mx = 10\n",
    "step = 0.01\n",
    "\n",
    "# artificial loss function and its gradient:\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "x = np.arange(-mx, mx, step) # TODO: change to w\n",
    "loss_function = a * x ** 2 + b * x + c\n",
    "gradient_loss_function = 2 * a * x + b\n",
    "\n",
    "# gradient descent:\n",
    "lr = 0.90\n",
    "epsilon = 0.0001 # stop when close enough to the minimum\n",
    "xi = mx / 2 # starting point\n",
    "n_iterations = 100 # learning steps\n",
    "xs = []\n",
    "ys = []\n",
    "iterations = []\n",
    "\n",
    "# introduce momentum:\n",
    "rho = 0.1\n",
    "grad_momentum = (2 * a * xi + b)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # store the current point and iteration:\n",
    "    iterations.append(i)\n",
    "    xs.append(xi)\n",
    "    \n",
    "    # perform a learning update:\n",
    "    grad_momentum = rho * grad_momentum + (1-rho) * (2 * a * xi + b)\n",
    "    xi = xi - lr * grad_momentum\n",
    "    \n",
    "    # store the loss\n",
    "    ys.append(a * xi ** 2 + b * xi + c)\n",
    "    \n",
    "    if(ys[i] < np.min(loss_function) + epsilon):\n",
    "        print(f'Iteration at convergence: {i}')\n",
    "        break\n",
    "        \n",
    "iterations = np.asarray(iterations)\n",
    "xs  = np.asarray(xs)\n",
    "ys = np.asarray(ys)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, loss_function, 'k')\n",
    "plt.scatter(xs, a * xs ** 2 + b * xs + c, c = iterations, cmap='viridis')\n",
    "plt.xlabel('$w$', fontsize=fs)\n",
    "plt.ylabel('$\\mathcal{L}(w)$', fontsize=fs)\n",
    "plt.title('Gradient descent, learning rate = ' + str(lr) + ' rho = ' + str(rho), fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd20ce5",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\"><B>Exercise 3.</B></FONT> <BR>\n",
    "Below, we use the commonly recommended parameter settings for $\\rho_1$ and $\\rho_2$. With these settings, the Adam algorithm does not necessarily shorten the convergence time, but makes the optimization much more robust against differences in learning rate, preventing divergence. It even works for a learning rate of $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate gradient descent, adam:\n",
    "mx = 10\n",
    "step = 0.01\n",
    "\n",
    "# whether to use adam or not:\n",
    "adam = True\n",
    "\n",
    "# adam parameters:\n",
    "rho1 = 0.9\n",
    "rho2 = 0.999\n",
    "epsilon = 1e-8\n",
    "s = 0\n",
    "r = 0\n",
    "\n",
    "# loss function:\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "x = np.arange(-mx, mx, step) # TODO: change to w\n",
    "loss_function = a * x ** 2 + b * x + c\n",
    "gradient_loss_function = 2 * a * x + b\n",
    "\n",
    "# gradient descent:\n",
    "lr = 0.02\n",
    "xi = mx / 2\n",
    "n_iterations = 100\n",
    "xs = []\n",
    "ys = []\n",
    "iterations = []\n",
    "for i in range(n_iterations):\n",
    "    iterations.append(i)\n",
    "    xs.append(xi)\n",
    "    if(not adam):\n",
    "        xi = xi - lr * (2 * a * xi + b)\n",
    "    else:\n",
    "        gradient = (2 * a * xi + b)\n",
    "        s = rho1 * s + (1 - rho1) * gradient\n",
    "        r = rho2 * r + (1 - rho2) * gradient ** 2\n",
    "        s_hat = s / (1 - rho1 ** (i + 1))\n",
    "        r_hat = r / (1 - rho2 ** (i + 1))\n",
    "        xi = xi - lr * s_hat / (np.sqrt(r_hat) + epsilon)\n",
    "\n",
    "    ys.append(a * xi ** 2 + b * xi + c)\n",
    "    if(ys[i] < np.min(loss_function) + epsilon):\n",
    "        print(f'Iteration at convergence: {i}')\n",
    "        break\n",
    "        \n",
    "iterations = np.asarray(iterations)\n",
    "xs  = np.asarray(xs)\n",
    "ys = np.asarray(ys)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, loss_function, 'k')\n",
    "# plt.plot(x, gradient_loss_function, 'r')\n",
    "# plt.plot(xs, a * xs ** 2 + b * xs + c, 'bo')\n",
    "plt.scatter(xs, a * xs ** 2 + b * xs + c, c = iterations, cmap='viridis')\n",
    "plt.xlabel('$w$', fontsize=fs)\n",
    "plt.ylabel('$\\mathcal{L}(w)$', fontsize=fs)\n",
    "plt.title('Gradient descent, learning rate = ' + str(lr), fontsize=fs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21862af6",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\"><B>Exercise 4.</B></FONT> <BR>\n",
    "    \n",
    "In the code below, we implement a low-pass filter for the validation loss, which stops as soon as the validation loss increases. As you can see, it overfits less to the noise in the training samples. The training loss is <i>higher</i>, but both the validation and test loss are <i>lower</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724df317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Whether to stop when the validation error increases:\n",
    "stop_when_val_error_increases = True\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "seed_number = 16012023\n",
    "torch.manual_seed(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "random.seed(seed_number)\n",
    "\n",
    "limit = 3\n",
    "n_steps = 1000\n",
    "x = torch.linspace(-limit, limit, n_steps)\n",
    "x = torch.reshape(x, (n_steps, 1))\n",
    "y_no_noise = 0.2 * x\n",
    "noise = 0.2 * torch.randn(x.shape)\n",
    "y = y_no_noise + noise\n",
    "\n",
    "# Split data into training, validation and test sets\n",
    "n_training = 10\n",
    "n_val = 30\n",
    "inds = np.random.choice(n_steps, n_training, replace=False)\n",
    "inds = np.sort(inds)\n",
    "x_training = x[inds]\n",
    "y_training = y[inds]\n",
    "remaining_inds = np.setdiff1d(np.arange(n_steps), inds)\n",
    "val_inds = np.random.choice(remaining_inds, n_val, replace=False)\n",
    "val_inds = np.sort(val_inds)\n",
    "x_val = x[val_inds]\n",
    "y_val = y[val_inds]\n",
    "remaining_inds = np.setdiff1d(remaining_inds, val_inds)\n",
    "remaining_inds = np.sort(remaining_inds)\n",
    "x_test = x[remaining_inds]\n",
    "y_test = y[remaining_inds]\n",
    "n_test = len(x_test)\n",
    "\n",
    "# Make a neural network model for the MLP with sigmoid activation functions in the hidden layer, and linear on the output\n",
    "n_hidden_neurons = 100\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                            torch.nn.Sigmoid(),torch.nn.Linear(n_hidden_neurons, 1))\n",
    "\n",
    "# We will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 1000\n",
    "show_plots = False\n",
    "show_plot_every = int(n_epochs / 5)\n",
    "training_loss = np.zeros(n_epochs)\n",
    "validation_loss = np.zeros(n_epochs)\n",
    "val_low_pass = 0\n",
    "\n",
    "for t in range(n_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_training)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_training)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    training_loss[t] = loss.item()\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Determine validation loss\n",
    "    y_val_pred = model(x_val)\n",
    "    val_loss = loss_fn(y_val_pred, y_val)\n",
    "    validation_loss[t] = val_loss.item()\n",
    "    prev_val = val_low_pass\n",
    "    if(t == 0):\n",
    "        val_low_pass = val_loss.item()\n",
    "    else:\n",
    "        val_low_pass = 0.9 * val_low_pass + 0.1 * val_loss.item()\n",
    "    if(stop_when_val_error_increases and t > 0 and val_low_pass > prev_val):\n",
    "        print('Validation loss increased, stopping training')\n",
    "        break\n",
    "\n",
    "    if show_plots and t % show_plot_every == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(x, y_no_noise, 'k--')\n",
    "        plt.plot(x_training, y_training, 'ko')\n",
    "        plt.plot(x_training, y_pred.detach().numpy())\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend(['Ground truth', 'Data', 'Fit'])\n",
    "        plt.show()\n",
    "\n",
    "# Plot the final state of affairs:\n",
    "\n",
    "mode_string = 'Validation set stopping' if stop_when_val_error_increases else 'No validation set'\n",
    "y_pred = model(x_training)\n",
    "y_pred_test = model(x_test)\n",
    "test_loss = loss_fn(y_pred_test, y_test)\n",
    "loss_string = \"{:.2f}\".format(test_loss.item())\n",
    "title_string = mode_string + ', test loss: ' + loss_string\n",
    "print('Test loss: ', test_loss.item())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y_no_noise, 'k--')\n",
    "plt.plot(x_training, y_training, 'ro')\n",
    "plt.plot(x_val, y_val, 'go')\n",
    "#plt.plot(x_test, y_test, 'x')\n",
    "plt.plot(x_training, y_pred.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ground truth', 'Training data', 'Validation data', 'Fit'])\n",
    "plt.title(title_string)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the loss over time:\n",
    "plt.figure()\n",
    "plt.plot(training_loss, 'r')\n",
    "plt.plot(validation_loss, 'g')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.show()\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# Plot the final state of affairs to compare SGD and Adam:\n",
    "plt.figure()\n",
    "plt.plot(x, y_no_noise, 'k--')\n",
    "plt.plot(x_training, y_training, 'ko')\n",
    "plt.plot(x_training, y_pred.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ground truth', 'Data', 'Fit'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred = model(x_training)\n",
    "loss = loss_fn(y_pred, y_training)\n",
    "print(f'Average loss on the training set: {loss.item()/n_training:.2f}')\n",
    "\n",
    "y_pred = model(x_val)\n",
    "loss = loss_fn(y_pred, y_val)\n",
    "print(f'Average loss on the validation set: {loss.item()/n_val:.2f}')\n",
    "\n",
    "y_pred = model(x_test)\n",
    "loss = loss_fn(y_pred, y_test)\n",
    "print(f'Average loss on the test set: {loss.item()/n_test:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a296b3",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\"><B>Exercise 5.</B></FONT>\n",
    "\n",
    "<OL>\n",
    "    <LI>An ensemble of 10 starts to give good results. Of course, for a definite answer, one should define an objective quantitive limit, like a value of the loss function.\n",
    "    </LI>\n",
    "</OL>\n",
    "\n",
    "<FONT COLOR=\"red\"><B>Exercise 6.</B></FONT>\n",
    "\n",
    "<OL>\n",
    "    <LI>Not as far as I know... For classification, one can interpret the magnitudes of the outputs as an uncertainty measure. For instance, if the highest activation of an output neuron is $0.3$ and the second highest is $0.2$, the classification is likely less certain than when the outputs are $1.0$ and $0.0$. However, for regression such a scheme cannot be used.</LI>\n",
    "    <LI>Dropout leads to more variance in the estimates outside of the training domain. The variance can be used as a measure of uncertainty. However, the magnitude of this variance is not strictly correlated with the magnitude of the error in that area.</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "drop_out = True\n",
    "\n",
    "# Create Tensors to hold input and target outputs.\n",
    "limit = 3\n",
    "n_steps = 1000\n",
    "x = torch.linspace(-limit, limit, n_steps)\n",
    "x = torch.reshape(x, (n_steps, 1))\n",
    "\n",
    "def GT_func(x, a=1, b= 0, c=0):\n",
    "    return a*x**2 + b*x + c\n",
    "a = 1\n",
    "b = 0\n",
    "c = 0\n",
    "y = GT_func(x, a, b, c)\n",
    "\n",
    "# Make a neural network model for the MLP with sigmoid activation functions in the hidden layer, and linear on the output\n",
    "n_hidden_neurons = 30\n",
    "p_drop_out = 0.2\n",
    "\n",
    "# Employ dropout or not:\n",
    "if(drop_out):\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                                torch.nn.Sigmoid(),torch.nn.Dropout(p=p_drop_out),\n",
    "                                torch.nn.Linear(n_hidden_neurons, 1))\n",
    "else:\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(1, n_hidden_neurons),\n",
    "                                torch.nn.Sigmoid(), torch.nn.Linear(n_hidden_neurons, 1))\n",
    "\n",
    "# Learning parameters:\n",
    "\n",
    "# We will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-5\n",
    "n_epochs = 10000\n",
    "\n",
    "show_intermediate_plots = False\n",
    "\n",
    "# Train:\n",
    "n_epochs_print = 1000\n",
    "im = 0\n",
    "for t in range(n_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    x_in = x\n",
    "    y_pred = model(x_in)\n",
    "    y_target = y\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_target)\n",
    "    if t % n_epochs_print == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    if t % n_epochs_print == 0 and show_intermediate_plots:\n",
    "        h = plt.figure()\n",
    "        plt.plot(x, y, 'k--')\n",
    "        plt.plot(x_in, y_pred.detach().numpy())\n",
    "        text = f'Epoch {t:4d}'\n",
    "        plt.text(0, 0.8*np.max(np.asarray(y)), text)\n",
    "        plt.show()\n",
    "\n",
    "# test the network also outside of the input domain in the training set:\n",
    "limit_factor = 2\n",
    "x_test = torch.linspace(-limit_factor*limit, limit_factor*limit, n_steps)\n",
    "x_test = torch.reshape(x_test, (n_steps, 1))\n",
    "\n",
    "if(drop_out):\n",
    "    n_samples = 30\n",
    "    y_test = torch.zeros((n_steps, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        yy = model(x_test)\n",
    "        y_test[:,i] = yy.squeeze()\n",
    "    y_test_mean = torch.mean(y_test, dim=1)\n",
    "else:\n",
    "    y_test_mean = model(x_test)\n",
    "    \n",
    "y_test_GT = GT_func(x_test, a, b, c)\n",
    "\n",
    "h = plt.figure()\n",
    "if(drop_out):\n",
    "    for i in range(n_samples):\n",
    "        plt.plot(x_test, y_test[:,i].detach().numpy(), 'bo', alpha=0.1)\n",
    "plt.plot(x_test, y_test_mean.detach().numpy(), 'r')\n",
    "plt.plot(x_test, y_test_GT, 'k--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d62e4",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\"><B>Exercise 7.</B></FONT> <BR>\n",
    "This is a quite open exercise. Higher performance can easily be obtained with, e.g., more training epochs. This does cost more time. \n",
    "    \n",
    "Below we show how you can add a convolutional layer. This requires recalculating the number of neurons after flattening. Whereas the initial image is $28 \\times 28 \\times 1$ pixels, after one convolution and max-pool, the resulting tensor has size $14 \\times 14 \\times 16$, and after our second convolution and max-pool $7 \\times 7 \\times 16 = 784$ neurons. Please remark that this is due to our settings, namely ``padding = 'same'`` and ``stride=2``. \n",
    "    \n",
    "Adding this convolutional layer actually <i>lowers</i> the performance after 10 epochs, but it may augment performance for longer learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8afc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        in_channel = 1\n",
    "        out_channel = 16\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding ='same') # bias = False\n",
    "        self.act1 = nn.ReLU(inplace= True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Extra convolutional layer:\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding ='same') # bias = False\n",
    "        self.act2 = nn.ReLU(inplace= True)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # We had to recalculate the number of neurons after flattening (784):\n",
    "        self.linear2 = nn.Linear(784,392)\n",
    "        self.act2 = nn.ReLU(inplace= True)\n",
    "        self.linear3 = nn.Linear(392, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.feature_maps = self.conv1(x)\n",
    "        tmp = self.act1(self.feature_maps)\n",
    "        tmp = self.maxpool1(tmp)\n",
    "        \n",
    "        # Extra convolutional layer:\n",
    "        tmp = self.conv2(tmp)\n",
    "        tmp = self.act2(tmp)\n",
    "        tmp = self.maxpool2(tmp)\n",
    "        \n",
    "        tmp = torch.flatten(tmp, 1, -1)\n",
    "        tmp = self.linear2(tmp)\n",
    "        tmp = self.act2(tmp)\n",
    "        logits = self.linear3(tmp)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbe774",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\"><B>Exercise 8.</B></FONT> <BR>\n",
    "In the original code, we set the learning rate to quite a normal value of 0.005. However, with that learning rate, it takes very long to learn the task. Even a 100 epochs give mediocre results. In fact, for this problem and learning setting, a much higher learning rate actually will work really well. Below, we set the learning rate to 0.1. This allows learning the auto-encoder problem in only 5 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an FCNN:\n",
    "model = FCNN()\n",
    "\n",
    "learning_rate = 1e-1\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train = True\n",
    "if(train):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_x, training_y, batch_size, model, loss_fn, optimizer)\n",
    "        test_loop(test_x, test_y, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d968e9",
   "metadata": {},
   "source": [
    "And then run the following code to show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a number of random samples with their reconstructions:\n",
    "n_samples = 5\n",
    "for sam in range(n_samples):\n",
    "    sample_idx = torch.randint(len(training_x), size=(1,)).item()\n",
    "    img = training_x[sample_idx]\n",
    "    img = torch.reshape(img, (1,1,28,28))\n",
    "    reconstr = model(img)\n",
    "    loss = loss_fn(img, reconstr).item()\n",
    "    print(f'Loss = {loss}')\n",
    "    show_img_and_reconstr(img, reconstr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
